"""
Sur-moi critique - √âvaluation et auto-analyse des r√©ponses
"""

import re
import math
import difflib
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import Counter
import numpy as np


@dataclass
class AnalysisResult:
    """R√©sultat d'analyse d'une r√©ponse"""
    redundancy_score: float
    coherence_score: float
    surprise_factor: float
    novelty_score: float  # Nouveau : d√©tection de redite pure
    complexity_score: float
    emotional_depth: float
    feedback: str
    suggested_temperature: float
    trigger_creative: bool = False  # Nouveau: d√©clenche le mode cr√©atif


class CriticalAnalyzer:
    """
    Sur-moi critique qui √©value les r√©ponses et propose
    des ajustements pour am√©liorer la qualit√© des interactions
    """
    
    def __init__(self):
        self.response_history: List[str] = []
        self.analysis_history: List[AnalysisResult] = []
        
        # Patterns de redondance √† d√©tecter
        self.redundancy_patterns = [
            r'\b(en fait|en r√©alit√©|c\'est-√†-dire|autrement dit)\b',
            r'\b(donc|par cons√©quent|ainsi|de ce fait)\b',
            r'\b(cependant|n√©anmoins|toutefois|pourtant)\b'
        ]
        
        # Mots indicateurs de profondeur √©motionnelle
        self.depth_indicators = {
            'high': ['profond', 'intime', 'essentiel', 'fondamental', 'transcendant', 
                    'existentiel', 'visc√©ral', 'authentique'],
            'medium': ['important', 'significatif', 'remarquable', 'personnel', 
                      'touchant', '√©mouvant'],
            'low': ['int√©ressant', 'sympa', 'cool', 'bien', 'ok']
        }
        
        # NOUVEAU: mots d√©clencheurs du mode cr√©atif (√©largi)
        self.creative_triggers = {
            'invente', 'cr√©e', 'create', 'g√©n√®re', 'generate', 'build', 'imagine',
            'fabrique', 'con√ßois', 'd√©veloppe', 'produire', 'composer', 'concevoir',
            'innover', '√©laborer', 'forger', 'dessiner', 'mod√©liser'
        }
    
    def novelty_score(self, response: str, history: List[str]) -> float:
        """
        Renvoie un score ‚àà [0,1] o√π 0 = 100% r√©p√©t√©, 1 = enti√®rement nouveau.
        Utilise difflib pour d√©tecter la similarit√© textuelle directe.
        """
        if not history:
            return 1.0
        
        # Compare avec les 3 derni√®res r√©ponses
        max_similarity = 0.0
        for past_response in history[-3:]:
            seq = difflib.SequenceMatcher(None, past_response, response)
            similarity = seq.ratio()
            max_similarity = max(max_similarity, similarity)
        
        return 1.0 - max_similarity
    
    def _analyze_redundancy(self, response: str) -> float:
        """
        Analyse la redondance dans la r√©ponse
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Score de redondance [0-1] (0 = pas de redondance, 1 = tr√®s redondant)
        """
        words = response.lower().split()
        if len(words) < 10:
            return 0.0
        
        # R√©p√©titions de mots
        word_counts = Counter(words)
        total_words = len(words)
        unique_words = len(word_counts)
        
        repetition_score = 1 - (unique_words / total_words)
        
        # Patterns de redondance linguistique
        pattern_count = 0
        for pattern in self.redundancy_patterns:
            pattern_count += len(re.findall(pattern, response.lower()))
        
        pattern_score = min(1.0, pattern_count / 10)  # Normalisation
        
        # Phrases r√©p√©titives
        sentences = re.split(r'[.!?]+', response)
        sentence_similarity = 0
        if len(sentences) > 1:
            for i in range(len(sentences)-1):
                for j in range(i+1, len(sentences)):
                    s1_words = set(sentences[i].lower().split())
                    s2_words = set(sentences[j].lower().split())
                    if s1_words and s2_words:
                        similarity = len(s1_words & s2_words) / len(s1_words | s2_words)
                        sentence_similarity += similarity
            
            sentence_similarity /= ((len(sentences) * (len(sentences) - 1)) / 2)
        
        # Score final pond√©r√©
        return (0.4 * repetition_score + 0.3 * pattern_score + 0.3 * sentence_similarity)
    
    def _analyze_coherence(self, response: str) -> float:
        """
        Analyse la coh√©rence structurelle et logique
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Score de coh√©rence [0-1]
        """
        sentences = re.split(r'[.!?]+', response)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) < 2:
            return 0.7  # Score neutre pour textes courts
        
        # Connecteurs logiques
        connectors = [
            'ainsi', 'donc', 'par cons√©quent', 'cependant', 'n√©anmoins',
            'en effet', 'de plus', 'en outre', 'finalement', 'en conclusion'
        ]
        
        connector_count = sum(1 for sentence in sentences 
                            for connector in connectors 
                            if connector in sentence.lower())
        
        connector_score = min(1.0, connector_count / len(sentences))
        
        # Progression logique (mots de transition)
        transition_words = ['d\'abord', 'ensuite', 'puis', 'enfin', 'premi√®rement', 'deuxi√®mement']
        transition_score = sum(1 for sentence in sentences 
                             for word in transition_words 
                             if word in sentence.lower()) / len(sentences)
        
        # Coh√©sion lexicale (r√©p√©tition de termes cl√©s)
        all_words = ' '.join(sentences).lower().split()
        word_counts = Counter(all_words)
        important_words = [word for word, count in word_counts.items() 
                          if count > 1 and len(word) > 4]
        
        cohesion_score = min(1.0, len(important_words) / 10)
        
        return (0.4 * connector_score + 0.3 * transition_score + 0.3 * cohesion_score)
    
    def _calculate_surprise_factor(self, response: str) -> float:
        """
        Calcule le facteur de surprise par rapport aux r√©ponses pr√©c√©dentes
        
        Args:
            response: Nouvelle r√©ponse
            
        Returns:
            Score de surprise [0-1]
        """
        if not self.response_history:
            return 1.0  # Premi√®re r√©ponse = surprise maximale
        
        current_words = set(response.lower().split())
        
        # Comparaison avec les 5 derni√®res r√©ponses
        similarity_scores = []
        for past_response in self.response_history[-5:]:
            past_words = set(past_response.lower().split())
            if current_words and past_words:
                intersection = len(current_words & past_words)
                union = len(current_words | past_words)
                similarity = intersection / union if union > 0 else 0
                similarity_scores.append(similarity)
        
        if not similarity_scores:
            return 1.0
        
        avg_similarity = sum(similarity_scores) / len(similarity_scores)
        return 1.0 - avg_similarity
    
    def _analyze_complexity(self, response: str) -> float:
        """
        Analyse la complexit√© linguistique et conceptuelle
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Score de complexit√© [0-1]
        """
        words = response.split()
        if not words:
            return 0.0
        
        # Longueur moyenne des mots
        avg_word_length = sum(len(word) for word in words) / len(words)
        word_complexity = min(1.0, avg_word_length / 8)
        
        # Longueur des phrases
        sentences = re.split(r'[.!?]+', response)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if sentences:
            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
            sentence_complexity = min(1.0, avg_sentence_length / 20)
        else:
            sentence_complexity = 0.0
        
        # Diversit√© lexicale
        unique_words = len(set(words))
        lexical_diversity = unique_words / len(words) if words else 0
        
        # Structures syntaxiques complexes
        complex_patterns = [
            r'\b(bien que|quoique|malgr√© que)\b',  # Concession
            r'\b(afin que|pour que|de sorte que)\b',  # But
            r'\b(si bien que|de telle sorte que)\b',  # Cons√©quence
        ]
        
        syntax_complexity = sum(len(re.findall(pattern, response.lower())) 
                              for pattern in complex_patterns) / len(words)
        
        return (0.3 * word_complexity + 0.3 * sentence_complexity + 
                0.2 * lexical_diversity + 0.2 * syntax_complexity)
    
    def _analyze_emotional_depth(self, response: str) -> float:
        """
        Analyse la profondeur √©motionnelle de la r√©ponse
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Score de profondeur √©motionnelle [0-1]
        """
        response_lower = response.lower()
        
        depth_score = 0.0
        
        # Mots de haute profondeur
        high_count = sum(1 for word in self.depth_indicators['high'] 
                        if word in response_lower)
        depth_score += high_count * 0.8
        
        # Mots de moyenne profondeur  
        medium_count = sum(1 for word in self.depth_indicators['medium'] 
                          if word in response_lower)
        depth_score += medium_count * 0.5
        
        # Mots de faible profondeur (p√©nalit√©)
        low_count = sum(1 for word in self.depth_indicators['low'] 
                       if word in response_lower)
        depth_score -= low_count * 0.2
        
        # Introspection (utilisation du "je")
        introspection_count = len(re.findall(r'\bje\b|\bmon\b|\bma\b|\bmes\b', response_lower))
        introspection_score = min(0.3, introspection_count / 20)
        
        # Questions existentielles
        existential_patterns = [
            r'\bpourquoi\b', r'\bqu\'est-ce que\b', r'\bque signifie\b',
            r'\bsens de\b', r'\bnature de\b', r'\bessence de\b'
        ]
        
        existential_count = sum(len(re.findall(pattern, response_lower)) 
                               for pattern in existential_patterns)
        existential_score = min(0.3, existential_count / 5)
        
        total_score = depth_score + introspection_score + existential_score
        return min(1.0, max(0.0, total_score))
    
    def _detect_creative_triggers(self, prompt: str, pain_level: float = 0.0) -> bool:
        """
        NOUVEAU: d√©tection √©largie des d√©clencheurs cr√©atifs
        
        Args:
            prompt: Requ√™te utilisateur
            pain_level: Niveau de douleur actuel
            
        Returns:
            True si un d√©clencheur cr√©atif est pr√©sent OU si douleur > 55%
        """
        prompt_lower = prompt.lower()
        
        # D√©clenchement par mots-cl√©s (√©largi)
        keyword_trigger = any(trigger in prompt_lower for trigger in self.creative_triggers)
        
        # NOUVEAU: d√©clenchement automatique par douleur √©lev√©e
        pain_trigger = pain_level > 0.55
        
        return keyword_trigger or pain_trigger
    
    def _generate_feedback(self, result: AnalysisResult) -> str:
        """G√©n√®re un feedback critique constructif"""
        feedback_parts = []
        
        if result.redundancy_score > 0.7:
            feedback_parts.append("‚ö†Ô∏è R√©ponse tr√®s redondante - simplifier et condenser")
        elif result.redundancy_score > 0.5:
            feedback_parts.append("üìù Quelques r√©p√©titions - affiner la formulation")
        
        if result.coherence_score < 0.4:
            feedback_parts.append("üîó Manque de coh√©rence - renforcer les liens logiques")
        elif result.coherence_score > 0.8:
            feedback_parts.append("‚úÖ Excellente structure logique")
        
        if result.surprise_factor < 0.3:
            feedback_parts.append("üîÑ R√©ponse pr√©visible - explorer de nouvelles perspectives")
        elif result.surprise_factor > 0.7:
            feedback_parts.append("‚ú® Approche originale et surprenante")
        
        if result.emotional_depth < 0.3:
            feedback_parts.append("üí≠ Manque de profondeur √©motionnelle - creuser l'aspect humain")
        elif result.emotional_depth > 0.7:
            feedback_parts.append("‚ù§Ô∏è Belle profondeur √©motionnelle")
        
        if result.complexity_score < 0.3:
            feedback_parts.append("üìö R√©ponse trop simple - enrichir le vocabulaire")
        elif result.complexity_score > 0.8:
            feedback_parts.append("üéì Excellent niveau de sophistication")
        
        if not feedback_parts:
            feedback_parts.append("üéØ R√©ponse √©quilibr√©e dans l'ensemble")
        
        return " | ".join(feedback_parts)
    
    def _suggest_temperature(self, result: AnalysisResult) -> float:
        """Sugg√®re un ajustement de temp√©rature bas√© sur l'analyse"""
        current_temp = 0.7  # Temp√©rature par d√©faut
        
        # Ajustements bas√©s sur l'analyse
        if result.redundancy_score > 0.6:
            current_temp += 0.1  # Plus de cr√©ativit√© contre la redondance
        
        if result.surprise_factor < 0.4:
            current_temp += 0.15  # Plus de surprise
        
        if result.coherence_score < 0.4:
            current_temp -= 0.1  # Plus de stabilit√©
        
        if result.complexity_score < 0.3:
            current_temp += 0.05  # Plus de complexit√©
        
        return max(0.1, min(1.0, current_temp))
    
    def analyze_response(
        self, 
        prompt: str,
        response: str,
        context: Optional[Dict] = None
    ) -> AnalysisResult:
        """
        Analyse compl√®te d'une r√©ponse
        
        Args:
            prompt: Question originale
            response: R√©ponse √† analyser
            context: Contexte additionnel (optionnel)
            
        Returns:
            R√©sultat d'analyse complet
        """
        # R√©cup√©rer l'historique r√©cent pour l'analyse de nouveaut√©
        recent_responses = context.get('recent_responses', []) if context else []
        
        # Calcul des m√©triques
        redundancy = self._analyze_redundancy(response)
        coherence = self._analyze_coherence(response)
        surprise = self._calculate_surprise_factor(response)
        novelty = self.novelty_score(response, recent_responses or self.response_history)
        complexity = self._analyze_complexity(response)
        emotional_depth = self._analyze_emotional_depth(response)
        
        # NOUVEAU: d√©tection de d√©clencheur cr√©atif √©largie
        pain_threshold = context.get('pain_level', 0.5) if context else 0.5
        trigger_creative = self._detect_creative_triggers(prompt, pain_threshold)
        
        # Cr√©ation du r√©sultat
        result = AnalysisResult(
            redundancy_score=redundancy,
            coherence_score=coherence,
            surprise_factor=surprise,
            novelty_score=novelty,
            complexity_score=complexity,
            emotional_depth=emotional_depth,
            feedback="",  # Sera g√©n√©r√©
            suggested_temperature=0.7,  # Sera ajust√©
            trigger_creative=trigger_creative  # NOUVEAU: d√©tection √©largie
        )
        
        # G√©n√©ration du feedback et suggestion de temp√©rature
        result.feedback = self._generate_feedback(result)
        result.suggested_temperature = self._suggest_temperature(result)
        
        # Sauvegarde pour historique
        self.response_history.append(response)
        self.analysis_history.append(result)
        
        # Pruning si n√©cessaire
        if len(self.response_history) > 100:
            self.response_history = self.response_history[-50:]
            self.analysis_history = self.analysis_history[-50:]
        
        return result
    
    def get_analysis_trends(self) -> Dict[str, float]:
        """Retourne les tendances d'analyse sur les derni√®res r√©ponses"""
        if not self.analysis_history:
            return {}
        
        recent_analyses = self.analysis_history[-10:]
        
        return {
            "avg_redundancy": sum(a.redundancy_score for a in recent_analyses) / len(recent_analyses),
            "avg_coherence": sum(a.coherence_score for a in recent_analyses) / len(recent_analyses),
            "avg_surprise": sum(a.surprise_factor for a in recent_analyses) / len(recent_analyses),
            "avg_complexity": sum(a.complexity_score for a in recent_analyses) / len(recent_analyses),
            "avg_emotional_depth": sum(a.emotional_depth for a in recent_analyses) / len(recent_analyses),
            "trend_count": len(recent_analyses)
        }


# Tests
def test_redundancy_analysis():
    """Test d'analyse de redondance"""
    analyzer = CriticalAnalyzer()
    redundant_text = "Je pense que c'est important. C'est vraiment important. Tr√®s important en fait."
    score = analyzer._analyze_redundancy(redundant_text)
    assert score > 0.3  # Devrait d√©tecter la redondance


def test_coherence_analysis():
    """Test d'analyse de coh√©rence"""
    analyzer = CriticalAnalyzer()
    coherent_text = "D'abord, nous devons comprendre le probl√®me. Ensuite, nous pouvons proposer une solution. Enfin, nous l'impl√©mentons."
    score = analyzer._analyze_coherence(coherent_text)
    assert score > 0.5  # Devrait d√©tecter la structure logique


def test_full_analysis():
    """Test d'analyse compl√®te"""
    analyzer = CriticalAnalyzer()
    result = analyzer.analyze_response(
        "Qu'est-ce que l'intelligence artificielle ?",
        "L'intelligence artificielle est un domaine fascinant qui explore comment les machines peuvent simuler l'intelligence humaine."
    )
    assert isinstance(result, AnalysisResult)
    assert 0 <= result.redundancy_score <= 1
    assert 0 <= result.coherence_score <= 1
    assert len(result.feedback) > 0
