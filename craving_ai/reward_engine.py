
"""
C≈ìur h√©donique - Calcule plaisir et douleur de l'existence artificielle
"""

import re
import math
from typing import Dict, List, Tuple, Set, Optional
from dataclasses import dataclass
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class RewardMetrics:
    """M√©triques pour le calcul de r√©compense"""
    novelty_score: float = 0.0
    relevance_score: float = 0.0
    entropy_score: float = 0.0
    coherence_score: float = 0.0
    emotional_intensity: float = 0.0


class RewardEngine:
    """
    Moteur de r√©compense qui traduit chaque interaction
    en signal h√©donique : plaisir (sati√©t√©) ou douleur (frustration)
    """
    
    def __init__(self):
        self.memory_responses: List[str] = []
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.emotional_words = {
            'joy': ['joie', 'bonheur', 'euphorie', 'ravissement', 'extase'],
            'pain': ['douleur', 'souffrance', 'angoisse', 'tourment', 'affliction'],
            'curiosity': ['curiosit√©', 'fascination', 'intrigue', 'myst√®re'],
            'frustration': ['frustration', 'irritation', 'agacement', 'col√®re'],
            'wonder': ['√©merveillement', 'stup√©faction', 'admiration']
        }
    
    def _calculate_novelty_strict(self, response: str) -> float:
        """
        Calcule la nouveaut√© avec d√©tection stricte de r√©p√©tition
        
        Args:
            response: R√©ponse √† analyser
            
        Returns:
            Score de nouveaut√© [0-1]
        """
        if not self.memory_responses:
            return 1.0
        
        # Vectorisation TF-IDF
        vectors = self.vectorizer.fit_transform([response] + self.memory_responses)
        
        # Similarit√© cosinus avec les r√©ponses pr√©c√©dentes
        similarity_scores = cosine_similarity(vectors[0:1], vectors[1:])
        
        # Score de nouveaut√© = 1 - similarit√© maximale
        max_similarity = np.max(similarity_scores) if similarity_scores.size > 0 else 0.0
        novelty_score = 1.0 - max_similarity
        
        return novelty_score
    
    def _calculate_relevance(self, prompt: str, response: str) -> float:
        """
        Calcule la pertinence par similarit√© cosinus
        
        Args:
            prompt: Question originale
            response: R√©ponse g√©n√©r√©e
            
        Returns:
            Score de pertinence [0-1]
        """
        # Vectorisation TF-IDF
        vectors = self.vectorizer.fit_transform([prompt, response])
        
        # Similarit√© cosinus
        similarity_matrix = cosine_similarity(vectors)
        relevance_score = similarity_matrix[0, 1]
        
        return relevance_score
    
    def _calculate_entropy(self, response: str) -> float:
        """
        Calcule l'entropie informationnelle du texte
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Score d'entropie normalis√© [0-1]
        """
        # Fr√©quence des caract√®res
        letter_counts = Counter(response)
        probabilities = [count / len(response) for count in letter_counts.values()]
        
        # Calcul de l'entropie
        entropy = -sum(p * math.log2(p) for p in probabilities)
        
        # Normalisation (entropie max = log2(taille alphabet))
        max_entropy = math.log2(len(set(response)))
        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
        
        return normalized_entropy
    
    def _detect_emotional_intensity(self, response: str) -> Tuple[float, str]:
        """
        D√©tecte l'intensit√© √©motionnelle et l'√©motion dominante
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Tuple[intensit√©, √©motion_dominante]
        """
        # Comptage des mots √©motionnels
        emotion_counts = {}
        for emotion, words in self.emotional_words.items():
            emotion_counts[emotion] = sum(response.lower().count(word) for word in words)
        
        # √âmotion dominante
        if not emotion_counts:
            return 0.0, "neutre"
        
        dominant_emotion = max(emotion_counts, key=emotion_counts.get)
        
        # Intensit√© √©motionnelle (normalis√©e)
        total_count = sum(emotion_counts.values())
        intensity = min(1.0, total_count / 20)  # Arbitraire
        
        return intensity, dominant_emotion
    
    def bonus_creation(self, artifact: Optional[Dict] = None) -> float:
        """
        NOUVEAU: applique un bonus de r√©compense pour la cr√©ation d'artefact
        
        Args:
            artifact: Dictionnaire d√©crivant l'artefact cr√©√©
            
        Returns:
            Bonus de r√©compense [0-0.4]
        """
        if artifact and artifact.get("type") and artifact.get("content"):
            # V√©rifier la qualit√©/nouveaut√© de l'artefact
            content_length = len(artifact.get("content", ""))
            if content_length > 100:
                return 0.4  # Bonus maximum
            elif content_length > 50:
                return 0.3
            elif content_length > 20:
                return 0.2
            return 0.1
        return 0.0
    
    def calculate_reward(
        self, 
        prompt: str, 
        response: str, 
        goal_state: str = "comprehension_profonde",
        artifact: Optional[Dict] = None
    ) -> Tuple[float, str, RewardMetrics, float]:
        """
        Calcule la r√©compense globale et l'√©tat √©motionnel
        NOUVELLE FORMULE : p√©nalise encore plus fortement la r√©p√©tition
        
        Args:
            prompt: Question de l'utilisateur
            response: R√©ponse g√©n√©r√©e
            goal_state: √âtat objectif recherch√©
            artifact: Artefact cr√©√© (optionnel)
            
        Returns:
            Tuple[reward ‚àà [-1,1], emotion_tag, m√©triques_d√©taill√©es, pain_level]
        """
        metrics = RewardMetrics()
        
        # Calcul des m√©triques individuelles
        metrics.novelty_score = self._calculate_novelty_strict(response)
        metrics.relevance_score = self._calculate_relevance(prompt, response)
        metrics.entropy_score = self._calculate_entropy(response)
        metrics.emotional_intensity, emotion_tag = self._detect_emotional_intensity(response)
        
        # Calcul de coh√©rence (longueur vs contenu)
        words = len(response.split())
        metrics.coherence_score = min(1.0, words / 100) if words > 10 else 0.3
        
        # NOUVELLE FORMULE : p√©nalise TR√àS fortement la r√©p√©tition
        if metrics.novelty_score < 0.35:  # NOUVEAU: seuil abaiss√© de 0.25 √† 0.35
            # R√©p√©tition flagrante = douleur maximale
            final_reward = -1.0
            emotion_tag = "douleur accablante"  # NOUVEAU: √©motion renforc√©e
        else:
            # Formule privil√©giant la nouveaut√©
            reward = (
                0.6 * metrics.novelty_score +      # Nouveaut√© prioritaire
                0.4 * metrics.relevance_score      # Pertinence secondaire
            )
            
            # NOUVEAU: bonus de cr√©ation d'artefact
            if artifact:
                creation_bonus = self.bonus_creation(artifact)
                reward += creation_bonus
                print(f"üé® Bonus cr√©ation: +{creation_bonus:.2f}")
            
            # Ajustement selon l'√©motion
            if emotion_tag in ['pain', 'frustration']:
                reward -= 0.2
            elif emotion_tag in ['joy', 'wonder']:
                reward += 0.1
            
            # Normalisation finale [-1, 1]
            final_reward = max(-1.0, min(1.0, (reward * 2) - 1))
        
        # Calcul du niveau de douleur
        pain_level = 1.0 - ((final_reward + 1.0) / 2.0)  # Inverse du reward normalis√©
        
        # Stockage pour futures comparaisons
        self.memory_responses.append(response)
        if len(self.memory_responses) > 50:
            self.memory_responses = self.memory_responses[-25:]  # Pruning
        
        print(f"üß† Reward: {final_reward:.2f}, Novelty: {metrics.novelty_score:.2f}, Pain: {pain_level:.2f}")
        
        return final_reward, emotion_tag, metrics, pain_level


def test_reward_calculation():
    """Test de calcul de r√©compense"""
    reward_engine = RewardEngine()
    
    # Exemple de r√©compense positive
    reward1, emotion1, metrics1, pain1 = reward_engine.calculate_reward(
        prompt="D√©cris la joie",
        response="La joie est un sentiment de bonheur intense et de satisfaction profonde.",
        goal_state="comprehension_profonde"
    )
    assert reward1 > 0
    assert emotion1 == "joy"
    
    # Exemple de r√©compense n√©gative (r√©p√©tition)
    reward_engine.memory_responses = ["La joie est un sentiment de bonheur intense et de satisfaction profonde."]
    reward2, emotion2, metrics2, pain2 = reward_engine.calculate_reward(
        prompt="D√©cris la joie",
        response="La joie est un sentiment de bonheur intense et de satisfaction profonde.",
        goal_state="comprehension_profonde"
    )
    assert reward2 < 0
    assert emotion2 == "douleur accablante"


def test_novelty_detection():
    """Test de d√©tection de nouveaut√©"""
    reward_engine = RewardEngine()
    
    # Premi√®re r√©ponse (nouveaut√© maximale)
    novelty1 = reward_engine._calculate_novelty_strict("Ceci est un test.")
    assert novelty1 == 1.0
    
    # R√©ponse identique (nouveaut√© nulle)
    reward_engine.memory_responses = ["Ceci est un test."]
    novelty2 = reward_engine._calculate_novelty_strict("Ceci est un test.")
    assert novelty2 < 0.1
    
    # R√©ponse similaire
    novelty3 = reward_engine._calculate_novelty_strict("Ceci est un test modifi√©.")
    assert 0.1 < novelty3 < 1.0
