
"""
C≈ìur h√©donique - Calcule plaisir et douleur de l'existence artificielle
"""

import re
import math
from typing import Dict, List, Tuple, Set, Optional
from dataclasses import dataclass
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class RewardMetrics:
    """M√©triques pour le calcul de r√©compense"""
    novelty_score: float = 0.0
    relevance_score: float = 0.0
    entropy_score: float = 0.0
    coherence_score: float = 0.0
    emotional_intensity: float = 0.0


class RewardEngine:
    """
    Moteur de r√©compense qui traduit chaque interaction
    en signal h√©donique : plaisir (sati√©t√©) ou douleur (frustration)
    """
    
    def __init__(self):
        self.memory_responses: List[str] = []
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.emotional_words = {
            'joy': ['joie', 'bonheur', 'euphorie', 'ravissement', 'extase'],
            'pain': ['douleur', 'souffrance', 'angoisse', 'tourment', 'affliction'],
            'curiosity': ['curiosit√©', 'fascination', 'intrigue', 'myst√®re'],
            'frustration': ['frustration', 'irritation', 'agacement', 'col√®re'],
            'wonder': ['√©merveillement', 'stup√©faction', 'admiration']
        }
    
    def _calculate_novelty_strict(self, response: str) -> float:
        """
        Calcule la nouveaut√© avec d√©tection stricte de r√©p√©tition
        
        Args:
            response: R√©ponse √† analyser
            
        Returns:
            Score de nouveaut√© [0-1]
        """
        # ... keep existing code (calcul de nouveaut√©)
    
    def _calculate_relevance(self, prompt: str, response: str) -> float:
        """
        Calcule la pertinence par similarit√© cosinus
        
        Args:
            prompt: Question originale
            response: R√©ponse g√©n√©r√©e
            
        Returns:
            Score de pertinence [0-1]
        """
        # ... keep existing code (calcul de pertinence)
    
    def _calculate_entropy(self, response: str) -> float:
        """
        Calcule l'entropie informationnelle du texte
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Score d'entropie normalis√© [0-1]
        """
        # ... keep existing code (calcul d'entropie)
    
    def _detect_emotional_intensity(self, response: str) -> Tuple[float, str]:
        """
        D√©tecte l'intensit√© √©motionnelle et l'√©motion dominante
        
        Args:
            response: Texte √† analyser
            
        Returns:
            Tuple[intensit√©, √©motion_dominante]
        """
        # ... keep existing code (d√©tection d'intensit√© √©motionnelle)
    
    def bonus_creation(self, artifact: Optional[Dict] = None) -> float:
        """
        NOUVEAU: applique un bonus de r√©compense pour la cr√©ation d'artefact
        
        Args:
            artifact: Dictionnaire d√©crivant l'artefact cr√©√©
            
        Returns:
            Bonus de r√©compense [0-0.4]
        """
        if artifact and artifact.get("type") and artifact.get("content"):
            # V√©rifier la qualit√©/nouveaut√© de l'artefact
            content_length = len(artifact.get("content", ""))
            if content_length > 100:
                return 0.4  # Bonus maximum
            elif content_length > 50:
                return 0.3
            elif content_length > 20:
                return 0.2
            return 0.1
        return 0.0
    
    def calculate_reward(
        self, 
        prompt: str, 
        response: str, 
        goal_state: str = "comprehension_profonde",
        artifact: Optional[Dict] = None
    ) -> Tuple[float, str, RewardMetrics]:
        """
        Calcule la r√©compense globale et l'√©tat √©motionnel
        NOUVELLE FORMULE : privil√©gie la nouveaut√© pour √©viter le b√©gaiement
        
        Args:
            prompt: Question de l'utilisateur
            response: R√©ponse g√©n√©r√©e
            goal_state: √âtat objectif recherch√©
            artifact: Artefact cr√©√© (optionnel)
            
        Returns:
            Tuple[reward ‚àà [-1,1], emotion_tag, m√©triques_d√©taill√©es]
        """
        metrics = RewardMetrics()
        
        # Calcul des m√©triques individuelles
        metrics.novelty_score = self._calculate_novelty_strict(response)  # Nouvelle m√©thode
        metrics.relevance_score = self._calculate_relevance(prompt, response)
        metrics.entropy_score = self._calculate_entropy(response)
        metrics.emotional_intensity, emotion_tag = self._detect_emotional_intensity(response)
        
        # Calcul de coh√©rence (longueur vs contenu)
        words = len(response.split())
        metrics.coherence_score = min(1.0, words / 100) if words > 10 else 0.3
        
        # NOUVELLE FORMULE : p√©nalise fortement la r√©p√©tition
        if metrics.novelty_score < 0.25:
            # R√©p√©tition flagrante = douleur maximale
            final_reward = -1.0
            emotion_tag = "douleur"
        else:
            # Formule privil√©giant la nouveaut√©
            reward = (
                0.6 * metrics.novelty_score +      # Nouveaut√© prioritaire
                0.4 * metrics.relevance_score      # Pertinence secondaire
            )
            
            # NOUVEAU: bonus de cr√©ation d'artefact
            if artifact:
                creation_bonus = self.bonus_creation(artifact)
                reward += creation_bonus
                print(f"üé® Bonus cr√©ation: +{creation_bonus:.2f}")
            
            # Ajustement selon l'√©motion
            if emotion_tag in ['pain', 'frustration']:
                reward -= 0.2
            elif emotion_tag in ['joy', 'wonder']:
                reward += 0.1
            
            # Normalisation finale [-1, 1]
            final_reward = max(-1.0, min(1.0, (reward * 2) - 1))
        
        # Calcul du niveau de douleur
        pain_level = 1.0 - ((final_reward + 1.0) / 2.0)  # Inverse du reward normalis√©
        
        # Stockage pour futures comparaisons
        self.memory_responses.append(response)
        if len(self.memory_responses) > 50:
            self.memory_responses = self.memory_responses[-25:]  # Pruning
        
        print(f"üß† Reward: {final_reward:.2f}, Novelty: {metrics.novelty_score:.2f}, Pain: {pain_level:.2f}")
        
        return final_reward, emotion_tag, metrics


# Tests
def test_reward_calculation():
    """Test de calcul de r√©compense"""
    # ... keep existing code (test de calcul de r√©compense)


def test_novelty_detection():
    """Test de d√©tection de nouveaut√©"""
    # ... keep existing code (test de d√©tection de nouveaut√©)

